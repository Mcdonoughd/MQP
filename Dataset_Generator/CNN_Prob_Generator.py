
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from skimage.transform import resize
import cv2
import keras
from keras.utils import to_categorical
import matplotlib.pyplot as plt
import keras_resnet.models
from keras import backend as K
import csv

'''
RESNET by Daniel Mcdonough
this runs resent classifier of the image based features, 

HOG,LOG,Gabor and raw crop


'''



input_shape=(32, 32,1)
num_classes=2
class_weight = {0: 3,1: 1.}
splits=10


# compile the CNN here
def compileCNN():
    x = keras.layers.Input(input_shape)
    model = keras_resnet.models.ResNet50(x, classes=num_classes)
    model.compile("adam","binary_crossentropy", ["accuracy",f1_m],)
    return model

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))


# Given the ing link data
# read each image and resize to 32x32
def collectImageData(data):
    for feature in range(0, 4):
        for idx, file in enumerate(data[:, feature]):
            print(file)
            img = cv2.imread(file,0)
            resized = resize(img, input_shape)
            data[idx, feature] = resized
    return data



def calcMetrics(confusion_matrix):
    TP = confusion_matrix[0,0]
    FP = confusion_matrix[0,1]
    FN = confusion_matrix[1,0]
    TN = confusion_matrix[1,1]

    precision = TP/(TP+FP)
    recall = TP/(TP+FN)

    if TP == 0:
        fscore = 0
    else:
        fscore = 2*((precision*recall)/(precision+recall))

    # false negative rate
    FNR = FN/(TP+FN)

    # false positive rate
    FDR = FP/(TP+FP)

    # specificity
    SPC = TN/(FN+TN)

    # accuracy
    ACC = (TP+TN)/(TP+FP+FN+TN)

    return fscore,FNR,FDR,SPC,ACC
# Gabor wavelet probs
# [[0.46815935, 0.035027485, 0.8356016, 2.150725e-05, 0.40254295, 1.01691724e-10, 0.11467552, 1.4156165e-05, 0.07142034, 0.35883674, 0.21416102, 0.13163415, 0.008749118, 0.11777563, 0.13314287, 0.037254237, 0.4165447, 0.0009829326, 0.10108069, 0.21576972, 0.26297602, 0.09858394, 0.12940387, 0.6207081, 0.22110656, 0.1796084, 0.39498886, 0.13666311, 0.004781767, 0.0010645629, 0.914313, 0.6723639, 0.008171914, 0.0027106882, 0.81779283, 0.22165331, 0.8626235, 0.119162224, 5.3304957e-05, 0.00012991272, 0.44704098, 7.0041057e-07, 0.11034985, 0.3523515, 0.040870126, 0.1526941, 0.40280187, 0.21397541, 0.13203037, 0.030767415, 0.023522291, 0.1787859, 0.20652992, 0.36453763, 0.14491516, 0.05419839, 0.105168834, 0.11707761, 0.07714363, 0.1912435, 0.082073174, 0.27970132, 0.13795918, 0.6524095, 0.06593531, 0.044842076, 0.28759006, 0.08966647, 0.062004957, 0.9184895, 0.6229915, 0.06767306, 0.049790107, 0.019527406, 0.52222383, 0.05356633, 0.22719826, 0.05150278, 0.18472771, 0.18796125, 0.29461932, 0.04030606, 0.15572827, 0.07283173, 0.041808087, 0.32320288, 0.21334483, 0.12404875, 0.32846606, 0.18304384, 0.14095464, 0.12259285, 0.093652986, 0.11289483, 0.023568558, 0.052842867, 0.078324, 0.18949714, 0.78799033, 0.39481238, 0.07575871, 0.42285654, 0.07619503, 0.05095585, 0.031902023, 0.013595739, 0.001591726, 0.99999726, 0.7458073, 0.9999988, 1.0, 1.0, 0.82403696, 0.002054677, 0.0041407538, 0.0034085242, 0.021772455, 0.09635462, 0.0014044183, 0.03145144, 0.014332354, 0.99811685, 0.00046150733, 0.0017084194, 0.00057232805, 0.0006484372, 0.00030132628, 0.025246592, 0.0063348296, 0.9825911, 0.08759722, 0.00038382108, 0.71588534, 0.99997807, 0.0030635418, 0.033224173, 0.81049025, 0.22172864, 0.0031535428, 0.005752673, 0.011019572, 0.81720924, 0.99891794, 0.9764598, 0.8178136, 0.99990773, 0.39822856, 0.0010365932, 0.00021713639, 0.114227034, 0.000967881, 0.00028727774, 0.00069235556, 0.00030446812, 0.0016011572, 0.57287115, 0.53831065, 0.93883216, 0.60405356, 0.0018663007, 0.003274839, 0.0014894734, 0.023768188, 0.87066627, 0.0014170064, 0.002223954, 0.60386944, 0.005851114, 0.026813854, 0.009761248, 0.008095115, 0.44196656, 0.0029648973, 0.14214724, 0.121780254, 0.002130944, 0.0064156014, 0.020611318, 0.004718324, 0.07519638, 0.0105830245, 0.0011321338, 0.0029124217, 0.0035116253, 0.6996612, 0.0028857149, 0.0067902277, 0.01101557, 9.47439e-05, 0.0019887506, 0.06552507, 0.0007368792, 0.008696262, 0.8742202, 0.0013237579, 0.29211023, 0.75789964, 0.01396559, 3.709872e-05, 0.1278841, 0.020542115, 0.001128404, 0.041003834, 0.34147924, 0.6839105, 0.15804212, 0.007550521, 0.0025353306, 0.005968815, 0.0010668158, 0.026001483, 0.9833167, 0.9761514, 0.99979454, 0.99806947, 0.9989593, 0.9551534, 0.015188692, 0.006885039, 0.11887059, 0.0025947676, 0.002163209, 0.0107100075, 0.0046948213, 0.016058203, 0.0010905816, 0.0035588343, 0.005280657, 0.0017109734, 0.0016932985, 0.0018677589, 0.089879, 0.0752716, 0.91792476, 0.10590977, 0.0042632297, 0.85689586, 0.6385066, 0.0064281854, 0.04383427, 0.6250135, 0.310882, 0.0005097276, 0.0706839, 0.13639791, 0.7231803, 0.22670852, 0.0003189572, 0.80691767, 0.001449259, 0.507445, 0.059065048, 0.0016238503, 0.0005401781, 0.00090846675, 0.019028801, 0.023599438, 0.07968009, 0.00078427355, 0.7221558, 0.8801523, 0.9778857, 0.84348136, 0.101528294, 0.06292841, 0.03063085, 0.039406873, 0.9732372, 0.27577454, 0.21516792, 0.951935, 0.0008536954, 0.08771164, 0.0010889124, 0.00022393353, 0.03541376, 0.012606756, 0.0008669817, 0.0027689238, 0.0033340408, 0.00085802976, 0.0010433547, 0.00621226, 0.00176884, 0.091086425, 0.00041543617, 0.06528928, 0.07424766, 0.9709321, 0.02502699, 0.00060060393, 0.0030807958, 0.00045530353, 0.14491294, 0.0011168916, 0.13342527, 0.3515551, 0.91666144, 0.27103233, 0.94596577, 0.9793331, 0.02671743, 0.00037734603, 0.02036255, 0.0022854288, 0.0007555878, 0.0013615282, 0.5959938, 0.87488943, 0.03107844, 0.13813059, 0.042966273, 0.026606094, 0.12605321, 0.019114023, 0.024415763, 0.9994894, 0.4009891, 0.6141442, 0.86737365, 0.97638273, 0.027029708, 0.273061, 0.5466514, 4.036966e-05, 0.017994365, 0.027766375, 2.0395644e-11, 0.17536843, 7.229873e-08, 0.31618032, 0.2528354, 0.0042880108, 0.064493164, 0.015484989, 0.050595433, 0.70654315, 0.6512412, 0.15562263, 0.017909778, 0.7080657, 0.7907984, 0.008146071, 0.04926767, 0.010624469, 0.013269534, 0.0031760475, 0.6928143, 0.0095070405, 0.3883135, 4.0074695e-09, 1.0764044e-10, 0.1446567, 1.8630283e-16, 0.16103975, 0.049779195, 0.09219183, 0.009278471, 0.028053809, 0.13947833, 0.020946583, 0.01708534, 0.017418165, 0.99384016, 0.9275869, 0.9962214, 0.99656326, 0.21064804, 0.072467744, 0.06598238, 0.035784453, 0.97880006, 0.057961084, 0.26522684, 0.9849582, 0.028923307, 0.14630724, 0.03887651, 0.009872829, 0.03839214, 0.07043459, 0.021124462, 0.17236958, 0.028577264, 0.012563825, 0.023067102, 0.0050371005, 0.004556769, 0.6338619, 0.0034808149, 0.6112218, 0.6484653, 0.99807215, 0.21997295, 0.0058835084, 0.18202403, 0.005392966, 0.01953712, 0.14548974, 0.76509255, 0.8555303, 0.87548643, 0.37376893, 0.98912, 0.73431134, 0.08719834, 0.0115815215, 0.0068474473, 0.068911105, 0.021670857, 0.011820643, 0.9992878, 0.7642323, 0.0695709, 0.09927081, 0.056824442, 0.0011400372, 0.0018405154, 0.0029533443, 0.99999976, 0.9962501, 0.95342976, 1.0, 0.98895276, 0.6542295, 0.015803626, 0.0018400818, 0.001475473, 0.00033356424, 0.0026107994, 9.353258e-09, 1.0, 0.0013515326, 0.023684692, 0.0010855534, 0.0013927639, 0.00090830366, 0.0015153693, 0.0014841879, 0.027333986, 0.0029770301, 0.83193064, 0.0024318658, 0.0016740408, 0.08144353, 1.0, 0.00015302916, 0.01300485, 0.9999645, 2.3173219e-07, 0.00087925175, 0.002904192, 0.0076666586, 0.9587557, 0.94493103, 0.035197433, 0.27388653, 1.0, 0.011806426, 0.0048535145, 0.0046103946, 0.00045911313, 0.003961826, 0.0009748646, 0.0010399054, 0.0004910986, 0.0018908053, 0.9493584, 0.033059113, 0.9954608, 0.99671304, 0.0024858473, 0.0016521676, 0.002745022, 0.0010898308, 0.9949536, 0.0019419036, 0.0017039282, 0.5997045, 0.0007936017, 0.023724383, 0.00089038146, 0.0032097218, 0.0051516453, 0.0040664827, 0.0010097274, 0.0037058126, 0.0007069279, 0.000532843, 0.002279708, 0.0018589089, 0.00077666814, 0.0023487906, 0.001714453, 0.0023422986, 0.0025688098, 0.9989955, 0.0013629996, 0.0014686328, 0.0013929517, 0.00051185716, 0.0029855443, 0.0010878744, 0.0020797225, 0.004829472, 0.9834109, 0.005635353, 0.9765048, 0.9515429, 0.009401277, 0.00092258357, 0.0017980874, 0.002403867, 0.0013174195, 0.001287322, 0.970386, 0.9738082, 0.00084906036, 0.0072852937, 0.0013154241, 1.0, 0.0025309066, 0.00079916744, 1.0, 0.097576216, 1.0, 1.0, 1.0, 0.025562385, 1.0, 0.0022156558, 0.0046010735, 1.0, 1.0, 1.0, 1.0, 0.4150211, 1.0, 0.0005293142, 0.009578845, 1.0, 0.0006970316, 0.0012534752, 1.0, 0.00048314047, 1.0, 1.0, 0.005080967, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.00046790796, 1.0, 0.047811497, 1.0, 1.0, 1.0, 1.0, 0.048640523, 0.0141484225, 1.0, 1.0, 0.99999833, 0.04048871, 0.007387417, 0.012110314, 1.0, 0.9301694, 0.982756, 0.36475682, 0.99045676, 0.044057917, 0.017624557, 0.04097732, 0.0077105477, 0.98321795, 0.8395351, 1.0, 0.98259974, 1.0, 0.91304994, 0.00539136, 1.0, 1.0, 0.013091037, 1.0, 1.0, 0.012036869, 1.0, 0.015039798, 1.0, 1.0, 0.00040891636, 1.0, 0.0003590041, 0.00036163622, 0.9922935, 0.0040049963, 1.0, 0.016865497, 1.0, 1.0, 0.0129195, 0.010772391, 0.07497085, 0.83661306, 0.022039786, 0.32618237, 0.927768, 0.016068604, 1.0, 1.0, 0.9999999, 1.0, 0.01506466, 0.95300335, 1.0, 0.58118856, 0.03223404, 0.0014409394, 0.00071831717, 0.00040011187, 0.0015627377, 0.26593626, 0.99780184, 0.7559353, 1.6969904e-06, 0.02842418, 0.9979723, 0.44887456, 0.0039379518, 0.08803101, 0.00033231583, 0.0002460688, 0.0025916647, 4.909757e-07, 0.071142934, 0.0004178372, 0.00041397245, 0.0030164018, 0.0021161484, 0.002710679, 0.013428292, 0.011827987, 0.0012900677, 0.9300621, 0.038478754, 0.7782739, 1.7273311e-06, 2.6376852e-05, 0.03940275, 0.0029632612, 5.537279e-09, 1.5354532e-07, 0.0020177462, 0.0012532305, 0.24369349, 0.9796816, 0.040836994, 2.0699048e-08, 0.9018316, 0.006797223, 0.99638927, 0.38941756, 0.024699738, 0.0008371885, 0.033137932, 0.0031399645, 0.020063935, 0.019084595, 0.0013845722, 0.99390644, 0.99745804, 0.9912305, 0.99376464, 0.002385577, 0.1102884, 0.01409736, 0.0016786299, 0.9874098, 0.05922648, 0.0032410044, 0.992347, 0.0003757952, 0.32754543, 0.00051284244, 0.00060315407, 0.0017685537, 0.010787238, 0.0027570794, 0.0011703672, 0.00076528423, 0.00026341807, 0.0038257302, 0.00050733204, 0.0018562585, 0.0010460553, 0.0009020224, 0.0008911314, 0.0009207842, 0.9966518, 0.0032175407, 0.00096869183, 0.06604704, 0.00054510206, 0.008154678, 0.00064377947, 0.006900795, 0.0062013334, 0.9964001, 0.013604656, 0.9936918, 0.9910233, 0.0045132427, 0.0004950586, 0.0008297315, 0.012664091, 0.0002462124, 0.0031267223, 0.98604035, 0.9887632, 0.0012262436, 0.0068636932, 0.0034639253, 0.00067760225, 0.00088527915, 0.21448107, 0.9911624, 0.9648067, 0.21377477, 1.0, 0.9102071, 0.001230792, 0.0063655037, 0.002163454, 0.029310549, 0.00073750026, 0.005466853, 0.016354559, 0.0053131753, 0.9999999, 0.0008911182, 0.0012743929, 0.0016669234, 0.0018543015, 0.01928828, 0.0059922305, 0.0023873977, 0.05409714, 0.0021372268, 0.004631153, 0.9999021, 0.9999615, 0.0018811255, 0.0072128666, 0.99490356, 0.87849027, 0.00085425645, 0.0024458219, 0.0013753808, 0.9065876, 0.06604534, 0.05818761, 0.028407242, 0.0041036983, 0.99305975, 0.0051010763, 0.0027494035, 0.0011635759, 0.0056558787, 0.0030943395, 0.002998215, 0.0024313245, 0.0121793235, 0.9946753, 0.997792, 0.99784994, 0.9972289, 0.0033264377, 0.007970511, 0.006802601, 0.001324804, 0.9911556, 0.15631257, 0.00084558054, 0.99664944, 0.0011985137, 0.02680288, 0.0057647764, 0.014917361, 0.0009909486, 0.0124519, 0.001251576, 0.0019016911, 0.0016027457, 0.00082203734, 0.0044785156, 0.004157124, 0.00043532587, 0.0020426589, 0.00076969847, 0.002478855, 0.0024027766, 0.9990926, 0.0019325275, 0.0138496915, 0.09097894, 0.0009713489, 0.0011511742, 0.002800799, 0.12630008, 0.0034251742, 0.9920608, 0.013332485, 0.99405473, 0.9985642, 0.0023203194, 0.0017300064, 0.005778759, 0.0039691674, 0.0031265453, 0.0009643014, 0.9872563, 0.4708615, 0.002416274, 0.0013961277, 0.5143499, 0.0016524759, 0.007129717, 0.8668738, 0.9836322, 0.9540911, 0.94118977, 0.9999844, 0.9832018, 0.13215299, 0.0031415531, 0.020432282, 0.50269014, 0.010842786, 0.647241, 0.0685423, 0.004581626, 0.12839822, 0.0012318146, 0.0042276275, 0.18236287, 0.0071695396, 0.005233171, 0.86710274, 0.003607358, 0.97901505, 0.78398985, 0.053926688, 0.8983505, 0.9722881, 0.8673042, 0.87204754, 0.01724681, 0.6807616, 0.014454842, 0.0035764873, 0.06543904, 0.953344, 0.3173332, 0.10325572, 0.8177584, 0.0318155, 0.9645502, 0.040413916, 0.0126360925, 0.053442728, 0.011470457, 0.021445224, 0.026289133, 0.6864069, 0.46663162, 0.9813421, 0.9831951, 0.9906442, 0.9910562, 0.22527035, 0.73791, 0.0041279364, 0.01933878, 0.9719802, 0.9230962, 0.001640126, 0.9470233, 0.01785797, 0.87001264, 0.36332, 0.6174711, 0.004765714, 0.004437178, 0.4001516, 0.020921271, 0.115471624, 0.0064109396, 0.8445814, 0.35721722, 0.0011677908, 0.0033793543, 0.011047043, 0.0034270985, 0.0033797445, 0.99246806, 0.0036805926, 0.5297675, 0.60696375, 0.0098437155, 0.43395144, 0.005454943, 0.026350336, 0.018199973, 0.99013734, 0.45331106, 0.97871417, 0.9919258, 0.30331203, 0.022251, 0.53406423, 0.37207696, 0.020413434, 0.1426747, 0.9823282, 0.8290743, 0.0076157846, 0.6644669, 0.00856592, 0.0014555216, 0.0063781017, 0.98294455, 0.9997327, 0.47013366, 0.9646923, 0.9999864, 0.99966013, 0.94421095, 0.0044777333, 0.016038792, 0.0010347137, 0.00028202657, 0.00073877233, 0.00302678, 0.62881094, 0.6150054, 0.001427869, 0.12765315, 0.0024403017, 0.022284409, 0.00795532, 0.0009269371, 0.0090395305, 0.9961842, 0.043798886, 0.98976946, 0.9974027, 0.98471224, 0.14616153, 0.011311162, 0.0023275653, 0.7673544, 0.00066175574, 0.008737836, 0.0035356055, 0.9963509, 0.02183132, 0.09517682, 0.9961436, 0.00044861162, 0.99970526, 0.75284237, 0.0012358103, 0.0009368816, 0.15203314, 0.069683194, 0.8098204, 0.006492661, 0.0017826875, 0.9995937, 0.9989543, 0.99972075, 0.9998196, 0.90069455, 0.3319774, 0.4041752, 0.0016056432, 0.999949, 0.9863426, 0.0037594636, 0.99976856, 0.011308928, 0.9929382, 0.0011627306, 0.0070727477, 0.0066992636, 0.01516112, 0.002428724, 0.0321764, 0.0090329265, 0.00054125214, 0.0034594922, 0.00085722044, 0.0012008573, 0.008494223, 0.0002748569, 0.009611031, 0.008703154, 0.99991274, 0.0027022262, 0.001900913, 0.001431055, 0.00022482073, 0.031428687, 0.0009618679, 0.9761673, 0.18087813, 0.99852175, 0.63309294, 0.9995565, 0.9997446, 0.008671615, 0.0019854212, 0.00036913378, 0.1731727, 0.00085994217, 0.0005444055, 0.9988863, 0.9989712, 0.0022686396, 0.08692606]]

def show_traingraph(history):
    # list all data in history
    # print(history.history.keys())
    # summarize history for accuracy
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()
    # summarize history for loss
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()

# Run the CNN
def runCNN(data):

    kf = KFold(n_splits=splits)

    Prob_dataset = []

    for feature in range(0, 4):

        # compute new model for each feature
        model = compileCNN()

        accuracy_sum = 0
        fscore_sum = 0
        FNR_sum = 0
        FDR_sum = 0
        SPC_sum = 0

        Probability_as_Feature = []

        for train, test in kf.split(data):
            train_size = len(train)
            test_size = len(test)

            x_train = np.empty((train_size,32, 32,1))
            y_train = to_categorical(data[train,4],num_classes=2)

            # compute class weight
            # class_weight[0] =  train_size / sum(data[train, 4])

            y_test = data[test, 4]

            for i in range(train_size):
                x_train[i] = data[i,feature]/255

            x_test = np.empty((test_size, 32, 32,1))
            for i in range(test_size):
                x_test[i] = data[i, feature]/255

            print("Training... ")
            hist = model.fit(x_train, y_train, batch_size=100, epochs=10, validation_split=0.33, shuffle= True, class_weight=class_weight,verbose=1)
            # show_traingraph(hist)

            probs = model.predict(x_test, batch_size=64)


            # test sets are done in sequential order, so we can do this here
            for prob in probs:
                Probability_as_Feature.append(prob[0])


            y_pred_bool = np.argmax(probs, axis=1).astype(int)

            y_actu = pd.Series(y_test, name='Actual')
            y_pred = pd.Series(y_pred_bool, name='Predicted')

            df_confusion = pd.crosstab(y_actu, y_pred)
            print(df_confusion)


            #Check if confusion matrix is 2x2
            empty_class = [0, 0]
            if '0' not in df_confusion.columns:
                df_confusion['0'] = empty_class
            if '1' not in df_confusion.columns:
                df_confusion['1'] = empty_class

            # Calc all metrics
            fscore, FNR, FDR, SPC, ACC = calcMetrics(np.asarray(df_confusion))
            accuracy_sum = accuracy_sum + ACC
            fscore_sum = fscore_sum + fscore
            FNR_sum = FNR_sum + FNR
            FDR_sum = FDR_sum + FDR
            SPC_sum = SPC_sum + SPC
        accuracy_average = accuracy_sum / splits
        fscore_avg = fscore_sum / splits
        FNR_avg = FNR_sum / splits
        FDR_avg = FDR_sum / splits
        SPC_avg = SPC_sum / splits


        print("\nFeature: " + str(feature))
        print("Average F-score: " + str(fscore_avg))
        print("Average Accuracy: " + str(accuracy_average))
        print("Average False Negative Rate: " + str(FNR_avg))
        print("Average Selectivity: " + str(SPC_avg))
        print("Average False Discovery Rate: " + str(FDR_avg)+"\n")

        stat_array = [feature,fscore_avg,accuracy_average,FNR_avg,SPC_avg,FDR_avg]


        with open("resnet_test_"+str(feature)+".csv", "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(stat_array)

        # print(Probability_as_Feature)
        Prob_dataset.append(Probability_as_Feature)

    return Prob_dataset




# NOTE we do not need to scale the features as we are using a decision based algorithm

dataset = pd.read_csv("./dataset_report_notebook.csv")
dataset= dataset.replace("Healthy",1)
dataset = dataset.replace("Damaged",0)

meta_headers = ['Cropped Frame','Original Frame', "HOG","Laplace of Gaussian","Gabor Wavelet","SIFT","Centroid_x","Centroid_y"]
CNN_HEADERS = ['Cropped Frame',"HOG","Laplace of Gaussian","Gabor Wavelet","True Classification"]
CNN_data = dataset[CNN_HEADERS]
CNN_data['Cropped Frame Location'] = dataset['Cropped Frame'].values
CNN_data = np.array(CNN_data)

# Shuffle the data so that the K folds are not to be influenced by the original frame
# np.random.shuffle(CNN_data)

shuffled_cropped_image_locations = CNN_data[:,5]
print(shuffled_cropped_image_locations)

CNN_imagedata = collectImageData(CNN_data)


Prob_dataset_as_lists = runCNN(CNN_imagedata)
print(Prob_dataset_as_lists)
# Create empty DF

Join_headers = ['Cropped Frame','Cropped Frame_prob',"HOG_prob","Laplace of Gaussian_prob","Gabor Wavelet_prob"]
my_df = pd.DataFrame(columns = Join_headers)
my_df['Cropped Frame'] = shuffled_cropped_image_locations
my_df['Cropped Frame_prob'] = Prob_dataset_as_lists[0]
my_df['HOG_prob'] = Prob_dataset_as_lists[1]
my_df['Laplace of Gaussian_prob'] = Prob_dataset_as_lists[2]
my_df['Gabor Wavelet_prob'] = Prob_dataset_as_lists[3]
# probs_as_df = pd.DataFrame(data=Prob_dataset,columns=CNN_HEADERS)

dataset = pd.merge(my_df,dataset, on=['Cropped Frame'])

print(dataset['Cropped Frame_prob'])


dataset.to_csv('Dataset_ensable_new.csv',index=False)

exit()





